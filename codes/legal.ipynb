{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚öñÔ∏è Legal QA Domain ‚Äì Low-Resource Survey Notebook\n",
    "\n",
    "This notebook accompanies the paper *\"QA Analysis in Medical and Legal Domains: A Survey of Data Augmentation in Low-Resource Settings\"* and focuses on the **legal domain**.\n",
    "\n",
    "We analyze and visualize the characteristics of various legal QA datasets, compute semantic similarity using domain-specific embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, load_dataset\n",
    "import datasets\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "import umap.umap_ as umap\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "import spacy\n",
    "import json\n",
    "import umap\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "\n",
    "## üîß NLTK & spaCy Setup\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')  \n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "## üß† Sentence Transformer Model Setup\n",
    "model_name = \"infly/inf-retriever-v1-1.5b\"\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "model_legal = SentenceTransformer(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device=\"cuda\" if use_gpu else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Load and Prepare QA Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MMLU\n",
    "def load_and_prepare_mmlu(*categories):\n",
    "    texts = []\n",
    "    for category in categories:\n",
    "        dataset = load_dataset(\"cais/mmlu\", category)\n",
    "        combined_dataset = concatenate_datasets(\n",
    "            [dataset[split] for split in [\"test\", \"validation\", \"dev\"] if split in dataset]\n",
    "        )\n",
    "        texts.extend(\n",
    "            f\"{row['question']}\\n\\n{row['choices'][row['answer']]}\"\n",
    "            for row in combined_dataset\n",
    "            if row['choices'] and row['answer'] is not None\n",
    "        )\n",
    "    return texts\n",
    "\n",
    "mmlu_categories = [\"international_law\", \"jurisprudence\", \"logical_fallacies\", \n",
    "                   \"moral_disputes\", \"moral_scenarios\", \"professional_law\", \n",
    "                   \"public_relations\", \"us_foreign_policy\"]\n",
    "\n",
    "mmlu_texts = load_and_prepare_mmlu(*mmlu_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PolicyQA\n",
    "def load_policyqa_json(file_path):\n",
    "    \"\"\"\n",
    "    Load a SQuAD-style JSON file (dev/test/train) and return a list of\n",
    "    \"<question>\\\\n\\\\n<first_answer_text>\" strings.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    texts = []\n",
    "    for article in data.get(\"data\", []):\n",
    "        for para in article.get(\"paragraphs\", []):\n",
    "            for qa in para.get(\"qas\", []):\n",
    "                question = qa.get(\"question\")\n",
    "                answers = qa.get(\"answers\", [])\n",
    "                if question and answers:\n",
    "                    answer_text = answers[0].get(\"text\")\n",
    "                    if answer_text:\n",
    "                        texts.append(f\"{question}\\n\\n{answer_text}\")\n",
    "    return texts\n",
    "\n",
    "policyqa = load_policyqa_json(\"data/policyqa.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PolicyQA\n",
    "def load_policyqa_csv(file_path):\n",
    "    df = pd.read_csv(file_path, delimiter=\"\\t\")  \n",
    "    df = df[df['Label'] == \"Relevant\"]\n",
    "    texts = [\n",
    "        f\"{row['Query']}\\n\\n{row['Segment']}\"\n",
    "        for _, row in df.iterrows()\n",
    "        if pd.notna(row['Segment'])   \n",
    "    ]\n",
    "    return texts\n",
    "\n",
    "privacyqa_texts = load_policyqa_csv(\"data/privacyqa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TruthfulQA\n",
    "truthful_df = pd.read_csv(\"data/TruthfulQA.csv\")\n",
    "\n",
    "truthful_texts = [\n",
    "    f\"{row['Question']}\\n\\n{row['Best Answer']}\"\n",
    "    for _, row in truthful_df.iterrows()\n",
    "    if pd.notnull(row['Question']) and pd.notnull(row['Best Answer'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mmlu_texts), len(truthful_texts), len(policyqa), len(privacyqa_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Generate Embeddings for Each QA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode each dataset using the SentenceTransformer model\n",
    "mmlu_embeddings = model_legal.encode(mmlu_texts, batch_size=4, device=\"cuda\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "truthful_embeddings = model_legal.encode(truthful_texts, batch_size=4, device=\"cuda\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "privacyqa_embeddings = model_legal.encode(privacyqa_texts, batch_size=4, device=\"cuda\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "policyqa_embeddings = model_legal.encode(policyqa, batch_size=4, device=\"cuda\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare each low-resource QA dataset to the parent corpus\n",
    "policyqa_similarity = cosine_similarity(mmlu_embeddings, policyqa_embeddings).flatten()\n",
    "truthful_similarity = cosine_similarity(mmlu_embeddings, truthful_embeddings).flatten()\n",
    "privacyqa_similarity = cosine_similarity(mmlu_embeddings, privacyqa_embeddings).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Cosine Similarity Distribution Between Parent and Target Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(policyqa_similarity, label=\"PolicyQA\", color=\"orange\", fill=True, bins=30, alpha=0.5)\n",
    "plt.hist(truthful_similarity, label=\"TruthfulQA\", color=\"green\", fill=True, bins=30, alpha=0.5)\n",
    "plt.hist(privacyqa_similarity, label=\"PrivacyQA\", color=\"red\", fill=True, bins=30, alpha=0.5)\n",
    "\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Density (Probability Density)\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Preprocessing & Vocabulary Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(text): \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 1]\n",
    "    return tokens\n",
    "\n",
    "def get_frequency_counter(texts):\n",
    "    c = Counter()\n",
    "    for txt in texts:\n",
    "        tokens = preprocess_and_tokenize(txt)\n",
    "        c.update(tokens)\n",
    "    return c\n",
    "\n",
    "# Frequency distributions\n",
    "freq_mmlu        = get_frequency_counter(mmlu_texts)\n",
    "freq_truthful   = get_frequency_counter(truthful_texts)\n",
    "freq_privacyqa   = get_frequency_counter(privacyqa_texts)\n",
    "freq_policyqa   = get_frequency_counter(policyqa)\n",
    "\n",
    "# Convert to vocab sets (for OOV / overlap analysis)\n",
    "vocab_mmlu        = set(freq_mmlu.keys())\n",
    "vocab_privacyqa  = set(freq_privacyqa.keys())\n",
    "vocab_policyqa  = set(freq_policyqa.keys())\n",
    "vocab_truthful   = set(freq_truthful.keys())\n",
    "\n",
    "new_vocab_policyqa  = vocab_policyqa  - vocab_mmlu\n",
    "new_vocab_truthful  = vocab_truthful  - vocab_mmlu\n",
    "new_vocab_privacyqa  = vocab_privacyqa  - vocab_mmlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"TruthfulQA\", \"PolicyQA\", \"PrivacyQA\"]\n",
    "vocab_sets = [new_vocab_truthful, new_vocab_policyqa, new_vocab_privacyqa]\n",
    "\n",
    "overlap_matrix = np.zeros((3,3))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        overlap_matrix[i, j] = len(vocab_sets[i] & vocab_sets[j])   \n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(overlap_matrix, annot=True, xticklabels=datasets, yticklabels=datasets, cmap=\"Blues\", fmt=\".0f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = np.vstack([\n",
    "    policyqa_embeddings,\n",
    "    truthful_embeddings,\n",
    "    privacyqa_embeddings\n",
    "])\n",
    "labels = (\n",
    "    [\"ParentQA\"] * len(mmlu_embeddings) +\n",
    "    [\"PolicyQA\"] * len(policyqa_embeddings) +\n",
    "    [\"TruthfulQA\"] * len(truthful_embeddings) +\n",
    "    [\"PrivacyQA\"] * len(privacyqa_embeddings)\n",
    "\n",
    ")\n",
    "\n",
    "def tokenize_corpus(texts):\n",
    "    return [token for doc in texts for token in preprocess_and_tokenize(doc)]\n",
    "\n",
    "parent_vocab = set(tokenize_corpus(mmlu_texts))\n",
    "for name, texts in [\n",
    "    (\"ParentQA\", mmlu_texts),\n",
    "    (\"PolicyQA\", policyqa),\n",
    "    (\"TruthfulQA\", truthful_texts),\n",
    "    (\"PrivacyQA\", privacyqa_texts)\n",
    "]:\n",
    "    vocab = set(tokenize_corpus(texts))\n",
    "    oov = vocab - parent_vocab\n",
    "    oov_rate = len(oov) / len(vocab)\n",
    "    print(f\"{name} ‚Äî vocab size: {len(vocab):5d}, OOV size: {len(oov):5d}, OOV rate: {oov_rate:.2%}\")\n",
    "    print(\"  ‚Üí exemples d'OOV:\", list(oov)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Entropy\n",
    "for name, texts in [\n",
    "    (\"ParentQA\", mmlu_texts),\n",
    "    (\"PolicyQA\", policyqa),\n",
    "    (\"TruthfulQA\", truthful_texts),\n",
    "    (\"PrivacyQA\",  privacyqa_texts)\n",
    "]:\n",
    "    tokens = tokenize_corpus(texts)\n",
    "    freq   = Counter(tokens)\n",
    "    ranks, counts = zip(*freq.most_common())\n",
    "    p = np.array(list(freq.values()), dtype=float)\n",
    "    p /= p.sum()\n",
    "    H = -np.sum(p * np.log2(p))\n",
    "    print(f\"{name} entropie Shannon: {H:.2f} bits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric=\"cosine\", n_components=2, random_state=42)\n",
    "proj = reducer.fit_transform(all_embeddings)\n",
    "\n",
    "palette = {\"ParentQA\":\"gray\",\"PolicyQA\":\"C0\",\"TruthfulQA\":\"C1\",\"PrivacyQA\":\"C2\"}\n",
    "colors  = [palette[l] for l in labels]\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "idx_parent = [i for i, l in enumerate(labels) if l == \"ParentQA\"]\n",
    "plt.scatter(proj[idx_parent, 0], proj[idx_parent, 1], c=palette[\"ParentQA\"], s=5, alpha=0.3, label=\"ParentQA\")\n",
    "\n",
    "for corpus, color in palette.items():\n",
    "    if corpus == \"ParentQA\":\n",
    "        continue\n",
    "    idx = [i for i, l in enumerate(labels) if l == corpus]\n",
    "    plt.scatter(proj[idx, 0], proj[idx, 1], c=color, s=5, alpha=0.6, label=corpus)\n",
    "\n",
    "plt.legend(markerscale=2)\n",
    "plt.title(\"Projection UMAP des embeddings\")\n",
    "plt.xlabel(\"UMAP‚Äê1\")\n",
    "plt.ylabel(\"UMAP‚Äê2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
