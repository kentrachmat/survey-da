{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🩺 Medical QA Domain – Low-Resource Survey Notebook\n",
    "\n",
    "This notebook accompanies the paper *\"QA Analysis in Medical and Legal Domains: A Survey of Data Augmentation in Low-Resource Settings\"* and focuses on the **medical domain**.\n",
    "\n",
    "We analyze and visualize the characteristics of various medical QA datasets, compute semantic similarity using domain-specific embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, load_dataset\n",
    "import datasets\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_venn import venn3\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "import umap.umap_ as umap\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import random\n",
    "import spacy\n",
    "import json\n",
    "import umap\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "\n",
    "## 🔧 NLTK & spaCy Setup\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')  \n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "## 🧠 Sentence Transformer Model Setup\n",
    "model_name = \"NovaSearch/jasper_en_vision_language_v1\"\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "model_medical = SentenceTransformer(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device=\"cuda\" if use_gpu else \"cpu\",\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.bfloat16 if use_gpu else torch.float32,\n",
    "        \"attn_implementation\": \"sdpa\"\n",
    "    },\n",
    "    config_kwargs={\"is_text_encoder\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📥 Load and Prepare QA Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MedMCQA\n",
    "medmcqa = datasets.load_dataset(\"openlifescienceai/medmcqa\")\n",
    "medmcqa_train = concatenate_datasets([medmcqa[\"train\"]])\n",
    "medmcqa_texts = []\n",
    "for ex in medmcqa_train:\n",
    "    q, e = ex.get(\"question\"), ex.get(\"exp\")\n",
    "    if q is None or e is None:\n",
    "        continue\n",
    "    \n",
    "    if e.startswith(\"Ans.\"):\n",
    "        e = e[4:].strip()  \n",
    "    text = f\"{q}\\n\\n{e}\"\n",
    "    medmcqa_texts.append(text)\n",
    "\n",
    "# Load CareQA\n",
    "careqa = load_dataset(\"HPAI-BSC/CareQA\", \"CareQA_en_open\")\n",
    "available_splits = careqa.keys()\n",
    "merged_ds = {\n",
    "    split: concatenate_datasets([careqa[split]]) for split in available_splits \n",
    "}\n",
    "\n",
    "careqa_texts = [\n",
    "    f\"{example['question']}\\n\\n{example['answer']}\"\n",
    "    for split in merged_ds.values()\n",
    "    for example in split\n",
    "]\n",
    "\n",
    "# Load COVID-QA\n",
    "with open(\"data/COVID-QA.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "covidqa_texts = [\n",
    "    f\"{qa['question']}\\n\\n{qa['answers'][0]['text']}\"\n",
    "    for entry in json_data['data']\n",
    "    for paragraph in entry['paragraphs']\n",
    "    for qa in paragraph['qas']\n",
    "    if qa['answers']  \n",
    "]\n",
    "\n",
    "# Load ReDis-QA \n",
    "redis_data = load_dataset(\"guan-wang/ReDis-QA\")\n",
    "option_map = {1: \"opa\", 2: \"opb\", 3: \"opc\", 4: \"opd\"}\n",
    "redisqa_texts = []\n",
    "for entry in redis_data['test']:\n",
    "    try:\n",
    "        if entry['cop'] == 0:\n",
    "            continue\n",
    "        correct_option = option_map.get(entry['cop'])  \n",
    "        if correct_option and correct_option in entry:   \n",
    "            redisqa_texts.append(f\"{entry['question']}\\n\\n{entry[correct_option]}\")\n",
    "        else:\n",
    "            print(f\"Skipping entry due to unexpected 'cop' value: {entry}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Skipping entry due to missing key: {e}\")\n",
    "\n",
    "# Load MedBullets\n",
    "df1 = pd.read_csv(\"data/medbullets_op4.csv\")\n",
    "df2 = pd.read_csv(\"data/medbullets_op5.csv\")\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "medbullets_texts = [\n",
    "    f\"{row['question']}\\n\\n{row['answer']}\"\n",
    "    for _, row in combined_df.iterrows()\n",
    "    if pd.notna(row['question']) and pd.notna(row['answer'])  \n",
    "]\n",
    "\n",
    "big_data = medmcqa_texts \n",
    "\n",
    "len(big_data), len(covidqa_texts), len(careqa_texts), len(redisqa_texts), len(medbullets_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Generate Embeddings for Each QA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode each dataset using the SentenceTransformer model\n",
    "covidqa_embeddings = model_medical.encode(covidqa_texts, show_progress_bar=True, device=\"cuda\")\n",
    "careqa_embeddings = model_medical.encode(careqa_texts, show_progress_bar=True, device=\"cuda\")\n",
    "redisqa_embeddings = model_medical.encode(redisqa_texts, show_progress_bar=True, device=\"cuda\")\n",
    "medbullets_embeddings = model_medical.encode(medbullets_texts, show_progress_bar=True, device=\"cuda\")\n",
    "parent_embeddings = model_medical.encode(big_data, show_progress_bar=True, device=\"cuda\")\n",
    "\n",
    "parent_embeddings.shape, covidqa_embeddings.shape, careqa_embeddings.shape, redisqa_embeddings.shape, medbullets_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare each low-resource QA dataset to the parent corpus\n",
    "covidqa_similarity = cosine_similarity(parent_embeddings, covidqa_embeddings).flatten()\n",
    "careqa_similarity = cosine_similarity(parent_embeddings, careqa_embeddings).flatten()\n",
    "redisqa_similarity = cosine_similarity(parent_embeddings, redisqa_embeddings).flatten()\n",
    "medbullets_similarity = cosine_similarity(parent_embeddings, medbullets_embeddings).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Cosine Similarity Distribution Between Parent and Target Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(covidqa_similarity, bins=30, alpha=0.5, label=\"CovidQA\", density=True)\n",
    "plt.hist(careqa_similarity, bins=30, alpha=0.5, label=\"CareQA\", density=True)\n",
    "plt.hist(redisqa_similarity, bins=30, alpha=0.5, label=\"ReDis-QA\", density=True)\n",
    "plt.hist(medbullets_similarity, bins=30, alpha=0.8, label=\"Medbullets\", density=True)\n",
    "\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Density (Probability Density)\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧹 Preprocessing & Vocabulary Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(text): \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words and len(t) > 1]\n",
    "    return tokens\n",
    "\n",
    "def get_frequency_counter(texts):\n",
    "    c = Counter()\n",
    "    for txt in texts:\n",
    "        tokens = preprocess_and_tokenize(txt)\n",
    "        c.update(tokens)\n",
    "    return c\n",
    "\n",
    "# Frequency distributions\n",
    "freq_big       = get_frequency_counter(big_data)\n",
    "freq_covidqa   = get_frequency_counter(covidqa_texts)\n",
    "freq_careqa    = get_frequency_counter(careqa_texts)\n",
    "freq_redisqa   = get_frequency_counter(redisqa_texts)\n",
    "freq_medbullets   = get_frequency_counter(medbullets_texts)\n",
    "\n",
    "# Convert to vocab sets (for OOV / overlap analysis)\n",
    "vocab_big       = set(freq_big.keys())\n",
    "vocab_covidqa   = set(freq_covidqa.keys())\n",
    "vocab_careqa    = set(freq_careqa.keys())\n",
    "vocab_redisqa   = set(freq_redisqa.keys())\n",
    "vocab_medbullets   = set(freq_medbullets.keys())\n",
    "\n",
    "new_vocab_covidqa = vocab_covidqa - vocab_big\n",
    "new_vocab_careqa  = vocab_careqa  - vocab_big\n",
    "new_vocab_redisqa = vocab_redisqa - vocab_big\n",
    "new_vocab_medbullets = vocab_medbullets - vocab_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"COVIDQA\", \"CareQA\", \"RedisQA\", \"MedBullets\"]\n",
    "vocab_sets = [new_vocab_covidqa, new_vocab_careqa, new_vocab_redisqa, new_vocab_medbullets]\n",
    "\n",
    "overlap_matrix = np.zeros((4, 4))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        overlap_matrix[i, j] = len(vocab_sets[i] & vocab_sets[j])   \n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(overlap_matrix, annot=True, xticklabels=datasets, yticklabels=datasets, cmap=\"Blues\", fmt=\".0f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = np.vstack([\n",
    "    covidqa_embeddings,\n",
    "    careqa_embeddings,\n",
    "    redisqa_embeddings,\n",
    "    medbullets_embeddings,\n",
    "    parent_embeddings\n",
    "])\n",
    "labels = (\n",
    "    [\"COVIDQA\"] * len(covidqa_embeddings) +\n",
    "    [\"CareQA\"]  * len(careqa_embeddings)  +\n",
    "    [\"ReDisQA\"] * len(redisqa_embeddings) +\n",
    "    [\"MedBullets\"] * len(medbullets_embeddings) +\n",
    "    [\"ParentQA\"] * len(parent_embeddings)\n",
    ")\n",
    "\n",
    "def tokenize_corpus(texts):\n",
    "    return [token for doc in texts for token in preprocess_and_tokenize(doc)]\n",
    "\n",
    "parent_vocab = set(tokenize_corpus(big_data))\n",
    "for name, texts in [\n",
    "    (\"ParentQA\",    big_data),\n",
    "    (\"COVIDQA\",    covidqa_texts),\n",
    "    (\"CareQA\",     careqa_texts),\n",
    "    (\"ReDisQA\",    redisqa_texts),\n",
    "    (\"MedBullets\", medbullets_texts),\n",
    "]:\n",
    "    vocab = set(tokenize_corpus(texts))\n",
    "    oov = vocab - parent_vocab\n",
    "    oov_rate = len(oov) / len(vocab)\n",
    "    print(f\"{name} — vocab size: {len(vocab):5d}, OOV size: {len(oov):5d}, OOV rate: {oov_rate:.2%}\")\n",
    "    print(\"  → exemples d'OOV:\", list(oov)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Entropy\n",
    "for name, texts in [\n",
    "    (\"ParentQA\",    big_data),\n",
    "    (\"COVIDQA\",     covidqa_texts),\n",
    "    (\"CareQA\",      careqa_texts),\n",
    "    (\"ReDisQA\",     redisqa_texts),\n",
    "    (\"MedBullets\",  medbullets_texts),\n",
    "]:\n",
    "    tokens = tokenize_corpus(texts)\n",
    "    freq   = Counter(tokens)\n",
    "    ranks, counts = zip(*freq.most_common())\n",
    "    p = np.array(list(freq.values()), dtype=float)\n",
    "    p /= p.sum()\n",
    "    H = -np.sum(p * np.log2(p))\n",
    "    print(f\"{name} entropie Shannon: {H:.2f} bits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, metric=\"cosine\", n_components=2, random_state=42)\n",
    "proj = reducer.fit_transform(all_embeddings)\n",
    "\n",
    "palette = {\"ParentQA\":\"gray\",\"COVIDQA\":\"C0\",\"CareQA\":\"C1\",\"ReDisQA\":\"C2\",\"MedBullets\":\"C3\"}\n",
    "colors  = [palette[l] for l in labels]\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "idx_parent = [i for i, l in enumerate(labels) if l == \"ParentQA\"]\n",
    "plt.scatter(proj[idx_parent, 0], proj[idx_parent, 1], c=palette[\"ParentQA\"], s=5, alpha=0.3, label=\"ParentQA\")\n",
    "\n",
    "for corpus, color in palette.items():\n",
    "    if corpus == \"ParentQA\":\n",
    "        continue\n",
    "    idx = [i for i, l in enumerate(labels) if l == corpus]\n",
    "    plt.scatter(proj[idx, 0], proj[idx, 1], c=color, s=5, alpha=0.6, label=corpus)\n",
    "\n",
    "plt.legend(markerscale=2)\n",
    "plt.title(\"Projection UMAP des embeddings\")\n",
    "plt.xlabel(\"UMAP‐1\")\n",
    "plt.ylabel(\"UMAP‐2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
