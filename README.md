# QA Analysis in Medical and Legal Domains: A Survey of Data Augmentation in Low-Resource Settings

## ğŸ“ Abstract

Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP), yet their success remains largely confined to highâ€‘resource, generalâ€‘purpose domains. Applying LLMs to **lowâ€‘resource, domainâ€‘specific QA** is challenging because of limited data, domain drift, and strict terminology constraints. This survey:

1. **Assesses dataset coverage** by comparing specialised QA sets against largeâ€‘scale â€œParentQAâ€ corpora.
2. **Reviews dataâ€‘centric boosts** such as synthetic task generation, retrievalâ€‘augmented generation, and corpus restructuring.
3. **Discusses evaluation & ethics**, including bias, privacy, and scalability.

Code and notebooks are provided for full reproducibility.

---

## ğŸ“‚ Repository Layout

```
â”œâ”€â”€ codes/                 
â”‚   â”œâ”€â”€ medical.ipynb     # Experiments in the biomedical domain
â”‚   â””â”€â”€ legal.ipynb       # Experiments in the legal domain
```

## ğŸš€ Quickâ€‘start

```bash
# 1. Clone the repo
$ git clone https://github.com/kentrachmat/survey-da.git
$ cd survey-da

# 2. Create env & install deps (Python â‰¥3.9)
$ python -m venv .venv && source .venv/bin/activate
$ pip install -r requirements.txt

# 3. Launch the notebooks
$ jupyter lab codes/medical.ipynb
```
 
## ğŸ“° News

- Paper accepted at (Poster Session) [**RJCRIâ€¯â€™25**](https://coria-taln-2025.lis-lab.fr/), Marseille
- Paper accepted at (Poster Session) [**ACLâ€‘SRWâ€¯â€™25**](https://acl2025-srw.github.io/), Vienna

<!-- ## ğŸ“š Citation

If you use this survey in your work, please cite:

TBA -->